{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t62FW3oyWOC6",
    "outputId": "bacc8aad-0cb8-446e-c41f-3a32d3f94b34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/dev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GL5Co5_zWOC-",
    "outputId": "45a1a365-0762-4c5f-da4a-b1727ddccb70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/dev/miniforge3/lib/python3.9/site-packages (0.0.2)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dev/miniforge3/lib/python3.9/site-packages (from bs4) (4.11.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dev/miniforge3/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGarW1n3WOC-"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyUkRomOWOC_",
    "outputId": "4e9581be-d2a2-4888-ac7b-38caf14eae24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/v8f_4jrn3xl3ljkf0f1bzmw80000gn/T/ipykernel_28668/3805039942.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1=pd.read_csv('data.tsv', sep='\\t', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv('data.tsv', sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hSv5BSQWODA"
   },
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sxXgw23UWODA"
   },
   "outputs": [],
   "source": [
    "df2 = df1[['star_rating', 'review_headline', 'review_body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzwnxv_6WODA"
   },
   "source": [
    " ## We form three classes and select 100,000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5LNxKlqWODB",
    "outputId": "6b910e2c-6c2f-453c-860c-3546388ea0d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Rating: 5\n",
      "Review Headline: So far so good\n",
      "Review Body: All of these cartridges were furnished by Amazon with not a faulty one in the lot.  Not having used all I ordered on that day, I can't know for sure.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Star Rating: 1\n",
      "Review Headline: One Star\n",
      "Review Body: Great Deal!!! Thanks!!\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Star Rating: 5\n",
      "Review Headline: LOVE EDWARD\n",
      "Review Body: Great keychain, brand new, looks great and arrived safe and sound, will be back again for more twilight itmes for sure\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Positive Reviews: 1848648, Negative Reviews: 415113, Neutral Reviews: 179871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_reviews=df2.loc[df2['star_rating'].isin([1,2])].sample(n=100000, random_state=36)\n",
    "negative_reviews=df2.loc[df2['star_rating'].isin([4,5])].sample(n=100000, random_state=36)\n",
    "processed_df = pd.concat([positive_reviews, negative_reviews])\n",
    "\n",
    "random_reviews = processed_df.sample(n=3, random_state=36)\n",
    "for index, row in random_reviews.iterrows():\n",
    "    print(f\"Star Rating: {row['star_rating']}\")\n",
    "    print(f\"Review Headline: {row['review_headline']}\")\n",
    "    print(f\"Review Body: {row['review_body']}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(f\"\\n\\n\\nPositive Reviews: {len(df2.loc[df2['star_rating'].isin([4,5])])}, Negative Reviews: {len(df2.loc[df2['star_rating'].isin([1,2])])}, Neutral Reviews: {len(df2.loc[df2['star_rating'].isin([3])])}\\n\")\n",
    "\n",
    "processed_df['label'] = processed_df['star_rating'].apply(lambda x: 1 if x > 3 else 0 if x < 3 else -1)\n",
    "positive_count = len(processed_df[processed_df['label'] == 1])\n",
    "negative_count = len(processed_df[processed_df['label'] == 0])\n",
    "neutral_count = len(processed_df[processed_df['label'] == -1])\n",
    "\n",
    "#Statistics of three classes (with comma between them)\n",
    "\n",
    "processed_df['review_headline'] = processed_df['review_headline'].apply(str)\n",
    "processed_df['review_body'] = processed_df['review_body'].apply(str)\n",
    "processed_df['review'] = processed_df[['review_headline', 'review_body']].agg(' '.join, axis=1)\n",
    "processed_df = processed_df.drop('review_headline', axis=1)\n",
    "processed_df = processed_df.drop('review_body', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxetwoA085Za",
    "outputId": "f2bd0e71-2ca2-47ae-b98d-0452898535bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Character count before Data Cleaning: 350.867275\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAverage Character count before Data Cleaning: {(processed_df['review'].str.len()).mean()}\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GabhnZFcWODB"
   },
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70QJAjnWWODB"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKu1WWWOAJrE",
    "outputId": "1162ffd4-2682-4176-cfd6-b666278ea8dc"
   },
   "outputs": [],
   "source": [
    "# Convert to lower case\n",
    "processed_df['review'] = processed_df['review'].str.lower()\n",
    "# Remove HTML tags\n",
    "processed_df['review'] = processed_df['review'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "# Remove URLs\n",
    "processed_df['review'] = processed_df['review'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "# Remove non-alphabetical characters\n",
    "processed_df['review'] = processed_df['review'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "# Remove extra spaces\n",
    "processed_df['review'] = processed_df['review'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# Performing contractions\n",
    "contractions = {\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"i will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "}\n",
    "\n",
    "processed_df['review'] = processed_df['review'].replace(contractions, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVV-0nACA3uJ",
    "outputId": "1084d0df-1b14-4191-da81-486b374f41ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Character count after Data Cleaning: 331.406895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAverage Character count after Data Cleaning: {(processed_df['review'].str.len()).mean()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJifIeRvWODC"
   },
   "source": [
    "## remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_iyg9GVGKT0",
    "outputId": "b1e9fb15-cc55-4511-e1b8-c9e12054d34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Character count before Data Preprocessing: 331.406895\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAverage Character count before Data Preprocessing: {(processed_df['review'].str.len()).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nuBV7OSWODC",
    "outputId": "59796881-95da-4920-c20d-e808ef986340"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#remove the stop words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_df['review'] = processed_df['review'].apply(lambda text: ' '.join([word for word in str(text).split() if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc5U0ZAjWODC"
   },
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5AMmp4htWODD"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "processed_df['review'] = processed_df['review'].apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i77uGgbq_BXD",
    "outputId": "5f568e0f-5600-4fe9-9567-a71bd1d87b30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Character count after Data Preprocessing: 214.03427\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAverage Character count after Data Preprocessing: {(processed_df['review'].str.len()).mean()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eto6F0DSQhpA",
    "outputId": "0880d807-8fc0-4d79-c57d-4b116ea10dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Label: 1\n",
      "Review: far good cartridge furnished amazon faulty one lot used ordered day cant know sure\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Label: 0\n",
      "Review: one star great deal thanks\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Label: 1\n",
      "Review: love edward great keychain brand new look great arrived safe sound back twilight itmes sure\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_reviews = processed_df.sample(n=3, random_state=36)\n",
    "\n",
    "# Print the random reviews\n",
    "print(\"\\n\\n\")\n",
    "for index, row in random_reviews.iterrows():\n",
    "    print(f\"Label: {row['label']}\")\n",
    "    print(f\"Review: {row['review']}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rMURZgenWODD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_df['review'], processed_df['label'], test_size=0.2, random_state=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niAuAvSqWODD"
   },
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7bKJwpzXD5sF"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "#Transform the test data using the same vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "#Now, X_train_tfidf and X_test_tfidf contain the TF-IDF features for training and testing sets\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "X_test_df = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jqoWuQKWODD"
   },
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Gub-yHQxWODD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.94970625\n",
      "Training Precision: 0.9421372483759229\n",
      "Training Recall: 0.9579729763605825\n",
      "Training F1-score: 0.9499891240172772\n",
      "\n",
      "\n",
      "Testing Accuracy: 0.896475\n",
      "Testing Precision: 0.8903510901762735\n",
      "Testing Recall: 0.9068651696508062\n",
      "Testing F1-score: 0.898532258459729\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#Create and train the Perceptron model\n",
    "perceptron_model = Perceptron(random_state=36)\n",
    "perceptron_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#Make predictions on training and testing data\n",
    "y_train_pred = perceptron_model.predict(X_train_tfidf)\n",
    "y_test_pred = perceptron_model.predict(X_test_tfidf)\n",
    "\n",
    "#Calculate metrics for training set\n",
    "train_accuracy_Perceptron = accuracy_score(y_train, y_train_pred)\n",
    "train_precision_Perceptron = precision_score(y_train, y_train_pred)\n",
    "train_recall_Perceptron = recall_score(y_train, y_train_pred)\n",
    "train_f1_Perceptron = f1_score(y_train, y_train_pred)\n",
    "\n",
    "#Calculate metrics for testing set\n",
    "test_accuracy_Perceptron = accuracy_score(y_test, y_test_pred)\n",
    "test_precision_Perceptron = precision_score(y_test, y_test_pred)\n",
    "test_recall_Perceptron = recall_score(y_test, y_test_pred)\n",
    "test_f1_Perceptron = f1_score(y_test, y_test_pred)\n",
    "\n",
    "#Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy_Perceptron}\")\n",
    "print(f\"Training Precision: {train_precision_Perceptron}\")\n",
    "print(f\"Training Recall: {train_recall_Perceptron}\")\n",
    "print(f\"Training F1-score: {train_f1_Perceptron}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy_Perceptron}\")\n",
    "print(f\"Testing Precision: {test_precision_Perceptron}\")\n",
    "print(f\"Testing Recall: {test_recall_Perceptron}\")\n",
    "print(f\"Testing F1-score: {test_f1_Perceptron}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVVHQQChWODD"
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9Jat9411WODD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.961475\n",
      "Training Precision: 0.9620763243786091\n",
      "Training Recall: 0.9606051490311098\n",
      "Training F1-score: 0.9613401738563241\n",
      "\n",
      "\n",
      "Testing Accuracy: 0.9235\n",
      "Testing Precision: 0.9257990867579908\n",
      "Testing Recall: 0.9225937283608665\n",
      "Testing F1-score: 0.9241936283010453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "#Create and train the SVM model\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#Make predictions on training and testing data\n",
    "y_train_pred = svm_model.predict(X_train_tfidf)\n",
    "y_test_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "#Calculate metrics for training set\n",
    "train_accuracy_SVM = accuracy_score(y_train, y_train_pred)\n",
    "train_precision_SVM = precision_score(y_train, y_train_pred)\n",
    "train_recall_SVM = recall_score(y_train, y_train_pred)\n",
    "train_f1_SVM = f1_score(y_train, y_train_pred)\n",
    "\n",
    "#Calculate metrics for testing set\n",
    "test_accuracy_SVM = accuracy_score(y_test, y_test_pred)\n",
    "test_precision_SVM = precision_score(y_test, y_test_pred)\n",
    "test_recall_SVM = recall_score(y_test, y_test_pred)\n",
    "test_f1_SVM = f1_score(y_test, y_test_pred)\n",
    "\n",
    "#Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy_SVM}\")\n",
    "print(f\"Training Precision: {train_precision_SVM}\")\n",
    "print(f\"Training Recall: {train_recall_SVM}\")\n",
    "print(f\"Training F1-score: {train_f1_SVM}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy_SVM}\")\n",
    "print(f\"Testing Precision: {test_precision_SVM}\")\n",
    "print(f\"Testing Recall: {test_recall_SVM}\")\n",
    "print(f\"Testing F1-score: {test_f1_SVM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neR0lJOEWODD"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KMrjiE37WODD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.93756875\n",
      "Training Precision: 0.9408962842234267\n",
      "Training Recall: 0.93343109974681\n",
      "Training F1-score: 0.9371488255909798\n",
      "\n",
      "\n",
      "Testing Accuracy: 0.924525\n",
      "Testing Precision: 0.9287103046014258\n",
      "Testing Recall: 0.9214066673261451\n",
      "Testing F1-score: 0.9250440698165208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#Create and train the Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#Make predictions on training and testing data\n",
    "y_train_pred = logreg_model.predict(X_train_tfidf)\n",
    "y_test_pred = logreg_model.predict(X_test_tfidf)\n",
    "\n",
    "#Calculate metrics for training set\n",
    "train_accuracy_logistic_regression = accuracy_score(y_train, y_train_pred)\n",
    "train_precision_logistic_regression = precision_score(y_train, y_train_pred)\n",
    "train_recall_logistic_regression = recall_score(y_train, y_train_pred)\n",
    "train_f1_logistic_regression = f1_score(y_train, y_train_pred)\n",
    "\n",
    "#Calculate metrics for testing set\n",
    "test_accuracy_logistic_regression = accuracy_score(y_test, y_test_pred)\n",
    "test_precision_logistic_regression = precision_score(y_test, y_test_pred)\n",
    "test_recall_logistic_regression = recall_score(y_test, y_test_pred)\n",
    "test_f1_logistic_regression = f1_score(y_test, y_test_pred)\n",
    "\n",
    "#Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy_logistic_regression}\")\n",
    "print(f\"Training Precision: {train_precision_logistic_regression}\")\n",
    "print(f\"Training Recall: {train_recall_logistic_regression}\")\n",
    "print(f\"Training F1-score: {train_f1_logistic_regression}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy_logistic_regression}\")\n",
    "print(f\"Testing Precision: {test_precision_logistic_regression}\")\n",
    "print(f\"Testing Recall: {test_recall_logistic_regression}\")\n",
    "print(f\"Testing F1-score: {test_f1_logistic_regression}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lecEZI_1WODD"
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TPyLkEB8WODD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.900075\n",
      "Training Precision: 0.9212493396724776\n",
      "Training Recall: 0.8743450903712617\n",
      "Training F1-score: 0.8971846021273039\n",
      "\n",
      "\n",
      "Testing Accuracy: 0.884125\n",
      "Testing Precision: 0.9093679398938685\n",
      "Testing Recall: 0.8560688495400138\n",
      "Testing F1-score: 0.8819138365901505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Create and train the Multinomial Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#Make predictions on training and testing data\n",
    "y_train_pred = nb_model.predict(X_train_tfidf)\n",
    "y_test_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "#Calculate metrics for training set\n",
    "train_accuracy_naive_bayes = accuracy_score(y_train, y_train_pred)\n",
    "train_precision_naive_bayes = precision_score(y_train, y_train_pred)\n",
    "train_recall_naive_bayes = recall_score(y_train, y_train_pred)\n",
    "train_f1_naive_bayes = f1_score(y_train, y_train_pred)\n",
    "\n",
    "#Calculate metrics for testing set\n",
    "test_accuracy_naive_bayes = accuracy_score(y_test, y_test_pred)\n",
    "test_precision_naive_bayes = precision_score(y_test, y_test_pred)\n",
    "test_recall_naive_bayes = recall_score(y_test, y_test_pred)\n",
    "test_f1_naive_bayes = f1_score(y_test, y_test_pred)\n",
    "\n",
    "#Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy_naive_bayes}\")\n",
    "print(f\"Training Precision: {train_precision_naive_bayes}\")\n",
    "print(f\"Training Recall: {train_recall_naive_bayes}\")\n",
    "print(f\"Training F1-score: {train_f1_naive_bayes}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy_naive_bayes}\")\n",
    "print(f\"Testing Precision: {test_precision_naive_bayes}\")\n",
    "print(f\"Testing Recall: {test_recall_naive_bayes}\")\n",
    "print(f\"Testing F1-score: {test_f1_naive_bayes}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
